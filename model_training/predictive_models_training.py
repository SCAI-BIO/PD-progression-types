import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, RepeatedStratifiedKFold, cross_val_score
from sklearn.metrics import roc_auc_score
from xgboost import XGBClassifier

# general settings
# all studies used
studies_train = ["ppmi", "iceberg", "luxpark"]

# PPMI is used for cross-cohort validation on iceberg and luxpark
studies_validate = ["iceberg", "luxpark"]

# 20 repeats for repeatet cross fold validation
n_repeat = 20

random_state = 1

methods = ["logistic", "randomforrest", "xgboost"]

# Outcomes used for predictive models. Compared to other analyses, we don't use UPDRS4 as is it mostly not assessed on baseline in PPMI.
cols_predict = ["UPDRS1", "UPDRS2", "UPDRS3", "PIGD", "MOCA", "SCOPA"] 

def load_prepare_data(data_bl: pd.DataFrame, cols_predict: list) -> (pd.DataFrame, pd.Series):
    """Splits a dataframe of baseline visits into features (X) and labels (y)

    Args:
        data_bl (pd.DataFrame): Dataframe of baseline visits including a column "cluster" and only one row (baseline visit) per patient.  
        cols_predict (list): List of outcomes which should be used as features in X. 

    Returns:
        (pd.DataFrame, pd.Series): Tuple (features, label) generated by splitting the data_bl DataFrame
    """
    # select correct columns, drop na rows, categorical classes
    data_bl = data_bl[cols_predict + ["cluster"]].dropna()
    data_bl["cluster"] = data_bl["cluster"].astype("category") 

    # create X and y
    X = data_bl.drop('cluster', axis=1)
    y = data_bl['cluster']

    return X, y

def load_prepare_data_w_followup(data_bl, data_fu, cols_predict):
    """Splits clinical data into features (X) and labels (y). The resulting objects are calculated from baseline visits (data_bl) and a one-year follow up visit (taken from data_fu).

    Args:
        data_bl (pd.DataFrame): Dataframe of baseline visits including a column "cluster" and only one row (baseline visit) per patient.  
        data_bl (pd.DataFrame): Dataframe of longitudinal visits including a column "cluster" and a cohort-specific column indicating the visits.
        cols_predict (list): List of outcomes which should be used as features in X. 

    Returns:
        (pd.DataFrame, pd.Series): Tuple (features, label) generated by splitting the DataFrames
    """
    # filter BL columns
    data_bl = data_bl[["Patient_ID"] + cols_predict + ["cluster"]]

    # filter 1 year follow up visit
    data_fu = data_fu[data_fu["Visit"] == "V04"]

    # filter FU columns 
    data_fu = data_fu[["Patient_ID"] + cols_predict]

    # calculate difference from 1-year follow up visit to baseline
    data_merged = pd.merge(data_bl, data_fu, on = "Patient_ID", suffixes=["", "_slope"], how = "inner") 
    data_merged.loc[:, map(lambda x: x + "_slope", cols_predict)] = data_merged[map(lambda x: x + "_slope", cols_predict)] - data_merged[cols_predict].values

    # drop na values
    data_merged = data_merged.dropna()

    # categorical cluster column
    data_merged["cluster"] = data_merged["cluster"].astype("category") 

    # reorder columns (this creates the order UPDRS1, UPDRS1_slope, UPDRS2, UPDRS2_solpe, ...)
    data_merged = data_merged[["Patient_ID", "cluster"] + list(map(lambda x: cols_predict[x // 2] if x % 2 == 0 else cols_predict[x // 2] + "_slope", range(0, len(cols_predict)*2)))]

    # create X and y
    X = data_merged.drop(['cluster', "Patient_ID"], axis=1)
    y = data_merged['cluster']

    return X, y

def logreg_nested_kfold(X: pd.DataFrame, y: pd.Series, n_repeat: int) -> dict:
    """Perform Logistic regression with L2 penalty hyperparameter optimization with inner and outer grid search and return best model.

    Args:
        X (pd.DataFrame): Dataframe containing features used for training. 
        y (pd.Series): Series cointaining labels which should be predicted. 
        n_repeat (int): How often inner and outer cross validation should be repeated. 

    Returns:
        dict: Dictionary with following keys: "outer_scores": ROC-AUC scores, "best_params": best parameters returned from grid search, "model": best model 
    """

    # Initialize Logistic Regression with L2 penalty and balanced class weights
    logreg = LogisticRegression(penalty='l2', class_weight='balanced', solver='liblinear', random_state=random_state)

    # Define hyperparameter space for Grid Search 
    param_grid = {'C': np.reciprocal([0.001, 0.01, 0.1, 1, 10, 100, 1000])}  # C is the inverse of lambda

    # Inner cross-validator: GridSearchCV with stratified k-fold CV
    cv_inner = RepeatedStratifiedKFold(n_splits=5, n_repeats=n_repeat, random_state=random_state)
    grid_search = GridSearchCV(logreg, param_grid, cv=cv_inner, scoring='roc_auc', n_jobs=-1)

    # Outer cross-validation to get mean + confidence interval for the ROC-AUC
    cv_outer = RepeatedStratifiedKFold(n_splits=5, n_repeats=n_repeat, random_state=random_state)
    outer_scores = cross_val_score(grid_search, X, y, cv=cv_outer, scoring='roc_auc')

    # Fit GridSearchCV on the training data
    grid_search.fit(X, y) 

    # Find the best hyperparameters
    best_params = grid_search.best_params_
    
    return {"outer_scores": outer_scores, "best_params": best_params, "model": grid_search}

def rf_nested_kfold(X: pd.DataFrame, y: pd.Series, n_repeat: int) -> dict:
    """Perform Random Forest hyperparameter optimization with inner and outer grid search and return best model.

    Args:
        X (pd.DataFrame): Dataframe containing features used for training. 
        y (pd.Series): Series cointaining labels which should be predicted. 
        n_repeat (int): How often inner and outer cross validation should be repeated. 

    Returns:
        dict: Dictionary with following keys: "outer_scores": ROC-AUC scores, "best_params": best parameters returned from grid search, "model": best model 
    """

    # Initialize Random Forest with balanced class weights
    rf = RandomForestClassifier(class_weight='balanced', random_state=random_state)

    # Define hyperparameter space for Grid Search 
    param_grid = {'bootstrap': [True, False],
                'max_depth': [10, 20, 30, 40],
                'min_samples_leaf': [1, 2, 4],
                'min_samples_split': [2, 5, 10],
                'n_estimators': [200, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}

    # Inner cross-validator: GridSearchCV with repeated stratified k-fold CV
    cv_inner = RepeatedStratifiedKFold(n_splits=5, n_repeats=n_repeat, random_state=random_state)
    grid_search = RandomizedSearchCV(rf, param_grid, cv=cv_inner, n_iter=50, scoring='roc_auc', n_jobs=-1)

    # Outer cross-validation to get mean + confidence interval for the ROC-AUC
    cv_outer = RepeatedStratifiedKFold(n_splits=5, n_repeats=n_repeat, random_state=random_state)
    outer_scores = cross_val_score(grid_search, X, y, cv=cv_outer, scoring='roc_auc')

    # Fit GridSearchCV on the training data
    grid_search.fit(X, y)

    # Find the best hyperparameters
    best_params = grid_search.best_params_

    return {"outer_scores": outer_scores, "best_params": best_params, "model": grid_search}

# XGBoost
def xgboost_nested_kfold(X: pd.DataFrame, y: pd.Series, n_repeat: int) -> dict:
    """Perform XGBoost hyperparameter optimization with inner and outer grid search and return best model.

    Args:
        X (pd.DataFrame): Dataframe containing features used for training. 
        y (pd.Series): Series cointaining labels which should be predicted. 
        n_repeat (int): How often inner and outer cross validation should be repeated. 

    Returns:
        dict: Dictionary with following keys: "outer_scores": ROC-AUC scores, "best_params": best parameters returned from grid search, "model": best model 
    """

    # Initialize XGBoost classifier with class weights
    scale_pos_weight = len(y[y == 0]) / len(y[y == 1])
    xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic', scale_pos_weight = scale_pos_weight, nthread=1, random_state=random_state)

    # hyperparameter space for XGBoost
    param_grid = {
        'min_child_weight': [1, 5, 10],
        'gamma': [0.5, 1, 1.5, 2, 5],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'max_depth': [3, 4, 5]
    }

    # Inner cross-validator: GridSearchCV with repeated stratified k-fold CV
    cv_inner = RepeatedStratifiedKFold(n_splits=5, n_repeats=n_repeat, random_state=random_state)
    grid_search = RandomizedSearchCV(xgb, param_grid, cv=cv_inner, n_iter = 50, scoring='roc_auc', n_jobs=-1)

    # Outer cross-validation to get mean + confidence interval for the ROC-AUC
    cv_outer = RepeatedStratifiedKFold(n_splits=5, n_repeats=n_repeat, random_state=random_state)
    outer_scores = cross_val_score(grid_search, X, y, cv=cv_outer, scoring='roc_auc')

    # Fit GridSearchCV on the training data
    grid_search.fit(X, y)

    # Find the best hyperparameters
    best_params = grid_search.best_params_

    return {"outer_scores": outer_scores, "best_params": best_params, "model": grid_search}

# Dataframes to store hyperparameter optimization results
df_crosscohortval_all = pd.DataFrame()
df_incohort_all = pd.DataFrame()

# load baseline and follow up data
data_bl = {}
data_fu = {}
for study in studies_train:
    data_bl[study] = pd.read_csv("data/" + study + "_bl.csv")
    data_fu[study] = pd.read_csv("data/" + study + "_visits.csv")

# perform hyperparameter optimization for all cohorts, baseline only / baseline + 1 visit follow up and cross-cohort validation
for use_fu in [0, 1]:
    for study in studies_train:
        # get features and labels, depending whether follow up data should be used
        if use_fu:
            X, y = load_prepare_data_w_followup(data_bl=data_bl[study], data_fu=data_fu[study], cols_predict=cols_predict)
        else:
            X, y = load_prepare_data(data_bl=data_bl[study], cols_predict=cols_predict)

        # perform hyperparameter optimization for all 3 methods
        for method in methods:
            if "logistic" == method:
                 res_test = logreg_nested_kfold(X = X, y = y, n_repeat = n_repeat)
            elif "randomforrest" == method:
                res_test = rf_nested_kfold(X = X, y = y, n_repeat = n_repeat)
            elif "xgboost" == method:
                res_test = xgboost_nested_kfold(X = X, y = y, n_repeat = n_repeat)
            else:
                raise ValueError()
            
            # store results and parameters
            df_incohort = pd.DataFrame({"roc_auc": res_test["outer_scores"]})
            df_incohort["best_params"] = str(res_test["best_params"])
            df_incohort["study_train"] = study
            df_incohort["study_val"] = study
            df_incohort["validation"] = 0
            df_incohort["model"] = str(res_test["model"])
            df_incohort["method"] = method
            df_incohort["FU"] = use_fu 
            df_incohort_all = pd.concat([df_incohort_all, df_incohort])

            # cross cohort validation using PPMI training
            if study == "ppmi":
                for study_val in studies_validate:
                    # get features and labels of validation cohort, depending whether follow up data should be used
                    if use_fu:
                        X_val, y_val = load_prepare_data_w_followup(data_bl=data_bl[study_val], data_fu=data_fu[study_val], cols_predict=cols_predict)
                    else:
                        X_val, y_val = load_prepare_data(data_bl=data_bl[study_val], cols_predict=cols_predict)

                    # use the PPMI-trained model
                    model_ppmi = res_test["model"]

                    # predict clusters using this model
                    y_pred_proba = model_ppmi.predict_proba(X_val)[:,1]

                    # calculate ROC AUC score
                    roc_auc = roc_auc_score(y_val, y_pred_proba)

                    # store results and parameters
                    df_crosscohortval = pd.Series({"roc_auc": roc_auc})
                    df_crosscohortval["best_params"] = str(res_test["best_params"])
                    df_crosscohortval["study_train"] = study
                    df_crosscohortval["study_val"] = study_val
                    df_crosscohortval["validation"] = 1
                    df_crosscohortval["model"] = str(res_test["model"])
                    df_crosscohortval["method"] = method
                    df_crosscohortval["FU"] = use_fu 
                    df_crosscohortval_all = pd.concat([df_crosscohortval_all, pd.DataFrame(df_crosscohortval).transpose()])